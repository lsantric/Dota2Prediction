{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luxxon/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Data/DotaTrainingData.txt\", sep = \"[\\n]|[,]\", names = [str(x) for x in range(11)])\n",
    "\n",
    "# Determining ratio of test and train datasets\n",
    "testSize, randomState = 0 , 1\n",
    "\n",
    "data_train, data_test, outcome_train, outcome_test = train_test_split(data.drop('10', axis=1), \n",
    "                                                                      data.iloc[:]['10'], \n",
    "                                                                      test_size = testSize, \n",
    "                                                                      random_state = randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 97)\n",
      "(15000, 97)\n",
      "(15000, 97)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.get_dummies(data_train.iloc[:,:5])\n",
    "df2 = pd.get_dummies(data_train.iloc[:,5:])\n",
    "\n",
    "df3 = pd.get_dummies(data_test.iloc[:,:5])\n",
    "df4 = pd.get_dummies(data_test.iloc[:,5:])\n",
    "\n",
    "df1.columns = df1.columns.map(lambda x: x[2:])\n",
    "df2.columns = df2.columns.map(lambda x: x[2:])\n",
    "df3.columns = df3.columns.map(lambda x: x[2:])\n",
    "df4.columns = df4.columns.map(lambda x: x[2:])\n",
    "\n",
    "featNum = np.unique(data_train).size\n",
    "\n",
    "df1 = df1.groupby(df1.columns, axis = 1).sum()\n",
    "df2 = df2.groupby(df2.columns, axis = 1).sum()\n",
    "df3 = df3.groupby(df3.columns, axis = 1).sum()\n",
    "df4 = df4.groupby(df4.columns, axis = 1).sum()\n",
    "\n",
    "\n",
    "features = pd.concat([df1, df2], axis=1)\n",
    "features_test = pd.concat([df3, df4], axis=1)\n",
    "\n",
    "print features.iloc[:, 0:97].shape\n",
    "print features.iloc[:, 97:].shape\n",
    "\n",
    "features = features.iloc[:, 0:97] - features.iloc[:, 97:]\n",
    "features_test = features_test.iloc[:, 0:97] - features_test.iloc[:, 97:]\n",
    "\n",
    "print features.shape\n",
    "print features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team interactions - min/max, shape\n",
      "1.0 -1.0 (15000, 4656)\n",
      "\n",
      "Counter interactions - min/max, shape\n",
      "1.0 -1.0 (15000, 4656)\n",
      "\n",
      "Selected team interactions: \n",
      "[array([1360, 2459, 2507, 2595, 2658, 2663, 2900, 3442, 3463, 3670])]\n",
      "\n",
      "Selected counter interactions: \n",
      "[array([ 211, 2466, 2471, 2904, 3343, 3530, 3669, 3680, 3686, 3706])]\n",
      "\n",
      "Final features shape:  (15000, 117)\n"
     ]
    }
   ],
   "source": [
    "logicv = LogisticRegressionCV(Cs=[0.08], fit_intercept=True, cv=100, penalty='l2', n_jobs=-1,\n",
    "                            solver='newton-cg', tol=0.0001, intercept_scaling=1.0, random_state = 1)\n",
    "\n",
    "polyF = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "new_features = features.values.copy()\n",
    "\n",
    "polyF.fit(features)\n",
    "\n",
    "new_features[new_features == -1] = 2\n",
    "new_features[new_features == 1] = 3\n",
    "\n",
    "# Creating same team pairs feature\n",
    "team_interactions = polyF.transform(new_features)\n",
    "\n",
    "team_interactions[team_interactions == 4] = -1\n",
    "team_interactions[team_interactions == 2] = 0\n",
    "team_interactions[team_interactions == 3] = 0\n",
    "team_interactions[team_interactions == 6] = 0\n",
    "team_interactions[team_interactions == 9] = 1\n",
    "\n",
    "team_interactions = team_interactions[:, 97:]\n",
    "\n",
    "print \"Team interactions - min/max, shape\"\n",
    "print np.max(team_interactions), np.min(team_interactions), team_interactions.shape\n",
    "\n",
    "# Creating counter team pairs\n",
    "new_features = np.multiply(\n",
    "    polyF.transform(new_features),\n",
    "    np.take(new_features, np.argmax(polyF.powers_, axis = 1), axis = 1)\n",
    ")\n",
    "\n",
    "counter_interactions = np.zeros_like(new_features)\n",
    "\n",
    "counter_interactions[new_features == 12] = -1\n",
    "counter_interactions[new_features == 18] = 1\n",
    "\n",
    "counter_interactions = counter_interactions[:, 97:] \n",
    "\n",
    "print \"\\nCounter interactions - min/max, shape\"\n",
    "print np.max(counter_interactions), np.min(counter_interactions), counter_interactions.shape\n",
    "\n",
    "# Selecting k best interaction by univariate correlation\n",
    "sK = SelectKBest(f_classif, k=10)\n",
    "sK.fit(team_interactions, outcome_train)\n",
    "\n",
    "print \"\\nSelected team interactions: \"\n",
    "print [sK.get_support(True)]\n",
    "\n",
    "team_indices = sK.get_support(True)\n",
    "\n",
    "# Selecting k best interaction by univariate correlation\n",
    "sK = SelectKBest(f_classif, k=10)\n",
    "sK.fit(counter_interactions, outcome_train)\n",
    "\n",
    "print \"\\nSelected counter interactions: \"\n",
    "print [sK.get_support(True)]\n",
    "\n",
    "counter_indices = sK.get_support(True)\n",
    "\n",
    "final_features = np.hstack((\n",
    "        features,\n",
    "        np.take(team_interactions, team_indices, axis = 1),\n",
    "        np.take(counter_interactions, counter_indices, axis = 1)\n",
    "    ))\n",
    "\n",
    "print \"\\nFinal features shape: \", final_features.shape\n",
    "\n",
    "logicv.fit(final_features, outcome_train.subtract(1))\n",
    "\n",
    "print \"Train accuracy achieved: \", logicv.score(final_features, outcome_train.subtract(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Serialising model parameters for submission\n",
    "pickle.dumps([logicv.C_, logicv.Cs_, logicv.coef_, logicv.intercept_, logicv.classes_]).encode(\"base64\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
